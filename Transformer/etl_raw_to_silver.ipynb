{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BCwiCTDoFGQ"
   },
   "source": [
    "# ETL (Extrair, Transformar e Carregar) da camada Raw para Silver\n",
    "Este notebook tem como objetivo aplicar, em nível moderado, a transformação e limpeza dos dados brutos da camada Raw para a camada Silver, a fim de fornecer uma “visão corporativa” da principal entidade do negócio. Ele processa os dados brutos do banco de dados da Comunicação de Acidente de Trabalho (CAT) do INSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DN9_jPdOt6sT"
   },
   "source": [
    "## 1. Extração\n",
    "Seção de extração dos dados brutos do arquivo CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waqynElL3A_j",
    "outputId": "fe78a4ef-87cf-45b6-9cec-5703b9631289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/yabamiah/Sandbox/cat-analytics/.venv/lib64/python3.14/site-packages (4.1.1)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /home/yabamiah/Sandbox/cat-analytics/.venv/lib64/python3.14/site-packages (from pyspark) (0.10.9.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PjjBpqbuQXe",
    "outputId": "a825195f-90c9-4c58-cd7d-93265e25171b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "26/01/12 18:29:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/12 18:29:34 WARN DependencyUtils: Local jar /home/yabamiah/Sandbox/cat-analytics/Transformer/path/to/postgresql-42.x.x.jar does not exist, skipping.\n",
      "26/01/12 18:29:34 INFO SparkContext: Running Spark version 4.1.1\n",
      "26/01/12 18:29:34 INFO SparkContext: OS info Linux, 6.17.12-300.fc43.x86_64, amd64\n",
      "26/01/12 18:29:34 INFO SparkContext: Java version 17.0.17+10\n",
      "26/01/12 18:29:34 INFO ResourceUtils: ==============================================================\n",
      "26/01/12 18:29:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "26/01/12 18:29:34 INFO ResourceUtils: ==============================================================\n",
      "26/01/12 18:29:34 INFO SparkContext: Submitted application: CSVtoPostgresETL\n",
      "26/01/12 18:29:34 INFO SecurityManager: Changing view acls to: yabamiah\n",
      "26/01/12 18:29:34 INFO SecurityManager: Changing modify acls to: yabamiah\n",
      "26/01/12 18:29:34 INFO SecurityManager: Changing view acls groups to: yabamiah\n",
      "26/01/12 18:29:34 INFO SecurityManager: Changing modify acls groups to: yabamiah\n",
      "26/01/12 18:29:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: yabamiah groups with view permissions: EMPTY; users with modify permissions: yabamiah; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "26/01/12 18:29:35 INFO Utils: Successfully started service 'sparkDriver' on port 35239.\n",
      "26/01/12 18:29:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "26/01/12 18:29:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "26/01/12 18:29:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "26/01/12 18:29:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "26/01/12 18:29:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "26/01/12 18:29:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c6d40c6c-08df-4cae-82d6-5872ce0569cb\n",
      "26/01/12 18:29:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "26/01/12 18:29:36 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "26/01/12 18:29:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/12 18:29:36 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "26/01/12 18:29:36 ERROR SparkContext: Failed to add path/to/postgresql-42.x.x.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/yabamiah/Sandbox/cat-analytics/Transformer/path/to/postgresql-42.x.x.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2185)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2241)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:544)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:544)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:544)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/01/12 18:29:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "26/01/12 18:29:36 INFO ResourceProfile: Limiting resource is cpu\n",
      "26/01/12 18:29:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "26/01/12 18:29:36 INFO SecurityManager: Changing view acls to: yabamiah\n",
      "26/01/12 18:29:36 INFO SecurityManager: Changing modify acls to: yabamiah\n",
      "26/01/12 18:29:36 INFO SecurityManager: Changing view acls groups to: yabamiah\n",
      "26/01/12 18:29:36 INFO SecurityManager: Changing modify acls groups to: yabamiah\n",
      "26/01/12 18:29:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: yabamiah groups with view permissions: EMPTY; users with modify permissions: yabamiah; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "26/01/12 18:29:36 INFO Executor: Starting executor ID driver on host fedora\n",
      "26/01/12 18:29:36 INFO Executor: OS info Linux, 6.17.12-300.fc43.x86_64, amd64\n",
      "26/01/12 18:29:36 INFO Executor: Java version 17.0.17+10\n",
      "26/01/12 18:29:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "26/01/12 18:29:36 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7bfbedb0 for default.\n",
      "26/01/12 18:29:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38501.\n",
      "26/01/12 18:29:36 INFO NettyBlockTransferService: Server created on fedora:38501\n",
      "26/01/12 18:29:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "26/01/12 18:29:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fedora, 38501, None)\n",
      "26/01/12 18:29:36 INFO BlockManagerMasterEndpoint: Registering block manager fedora:38501 with 434.4 MiB RAM, BlockManagerId(driver, fedora, 38501, None)\n",
      "26/01/12 18:29:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fedora, 38501, None)\n",
      "26/01/12 18:29:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fedora, 38501, None)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Agente  Causador  Acidente: string (nullable = true)\n",
      " |-- Data Acidente1: string (nullable = true)\n",
      " |-- CBO: string (nullable = true)\n",
      " |-- CID-10: string (nullable = true)\n",
      " |-- CNAE2.0 Empregador: integer (nullable = true)\n",
      " |-- CNAE2.0 Empregador Descrição: string (nullable = true)\n",
      " |-- Emitente CAT: string (nullable = true)\n",
      " |-- Espécie do benefício: string (nullable = true)\n",
      " |-- Filiação Segurado: string (nullable = true)\n",
      " |-- Indica Óbito Acidente: string (nullable = true)\n",
      " |-- Munic Empr: string (nullable = true)\n",
      " |-- Natureza da Lesão: string (nullable = true)\n",
      " |-- Origem de Cadastramento CAT: string (nullable = true)\n",
      " |-- Parte Corpo Atingida: string (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Tipo do Acidente: string (nullable = true)\n",
      " |-- UF  Munic.  Acidente: string (nullable = true)\n",
      " |-- UF Munic. Empregador: string (nullable = true)\n",
      " |-- Data Acidente18: string (nullable = true)\n",
      " |-- Data Despacho Benefício: string (nullable = true)\n",
      " |-- Data Acidente20: string (nullable = true)\n",
      " |-- Data Nascimento: string (nullable = true)\n",
      " |-- Data Emissão CAT: string (nullable = true)\n",
      " |-- CNPJ/CEI Empregador: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVtoPostgresETL\") \\\n",
    "    .config(\"spark.jars\", \"path/to/postgresql-42.x.x.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../Data layer/raw/dados_brutos.csv\")\n",
    "\n",
    "df.head()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGW9E_yauQlx"
   },
   "source": [
    "## 2. Transformação\n",
    "Nesta seção, iremos transformar, tratar e limpar os dados vindos da camada Raw para a camada Silver, ainda utilizando o Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru7PbKkiaQRy"
   },
   "source": [
    "### 2.1 Importações e configuração\n",
    "Aqui iremos importar o necessário do pyspark, definir algumas variáveis para auxiliar nas transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fyKon0BgaPmL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, sum as spark_sum\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "nao_classif_string = '{ñ class}'\n",
    "nao_classif_numeric = 0\n",
    "\n",
    "limite_repeticao = 0.95\n",
    "limite_nulos = 0.95\n",
    "\n",
    "total_linhas = df.count()\n",
    "colunas_a_remover = []\n",
    "novas_colunas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transformando Não Classificados para Nulos\n",
    "Fazendo a transformação de valores não classificados para nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in df.columns:\n",
    "    tipo_coluna = df.schema[coluna].dataType\n",
    "\n",
    "    # Fazemos isso pois algumas colunas possuem '.', o que buga\n",
    "    coluna_escaped = col(f\"`{coluna}`\")\n",
    "\n",
    "    if isinstance(tipo_coluna, StringType):\n",
    "        expressao = when(\n",
    "            coluna_escaped == nao_classif_string, None\n",
    "        ).otherwise(coluna_escaped).alias(coluna)\n",
    "    else:\n",
    "        expressao = when(coluna_escaped == nao_classif_numeric, None\n",
    "        ).otherwise(coluna_escaped).alias(coluna)\n",
    "\n",
    "    novas_colunas.append(expressao)\n",
    "\n",
    "# Executando as expressões de trocar dos valores não classificados para nulos\n",
    "df_limpo = df.select(*novas_colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUO27wt4azyQ"
   },
   "source": [
    "### 2.3 Remoção de Colunas com Dados Não Classificados ou Constante\n",
    "Aqui iremos remover as colunas que não agregam valor para a análise. Sendo as características utilizadas para as colunas caírem nesse termo são:\n",
    "\n",
    "- 95%+ dos valores são iguais\n",
    "- 60%+ dos valores são nulos/não classificados\n",
    "- Variância = 0\n",
    "- Não agrega informação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FouQGuPRvXKe",
    "outputId": "a9d989bd-a507-47b8-8937-093c204630c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo Emitente CAT: 95.82% de valor repetido.\n",
      "Removendo Espécie do benefício: 100.00% de valor repetido.\n",
      "Removendo Filiação Segurado: 99.13% de valor repetido.\n",
      "Removendo Indica Óbito Acidente: 99.54% de valor repetido.\n",
      "Removendo Origem de Cadastramento CAT: 100.00% de valor repetido.\n",
      "Removendo Data Despacho Benefício: 100.00% de valor repetido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo CNPJ/CEI Empregador: 96.25% de valor repetido.\n",
      "\n",
      "Total removido: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Agente  Causador  Acidente='Ataque de Ser Vivo por Mordedura, Picada, Chi', Data Acidente1='2023/04', CBO='621005-Trab. Agropecuário em Geral', CID-10='M23.8 Outr Transt Internos do Joelho', CNAE2.0 Empregador=None, CNAE2.0 Empregador Descrição=None, Munic Empr='000000-Ignorado', Natureza da Lesão='Lesao Imediata', Parte Corpo Atingida='Joelho', Sexo='Masculino', Tipo do Acidente='Típico', UF  Munic.  Acidente='Zerado', UF Munic. Empregador='Zerado', Data Acidente18='2023/04', Data Acidente20='24/04/2023', Data Nascimento='05/06/1977', Data Emissão CAT='01/05/2023')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for coluna in df_limpo.columns:\n",
    "    # Fazemos isso pois algumas colunas possuem '.', o que buga\n",
    "    coluna_escaped = f\"`{coluna}`\"\n",
    "\n",
    "    # 1. Verifica Porcentagem de Nulos\n",
    "    # Como já retiramos os não classificados, agora só contar os nulos diretamente\n",
    "    qtd_nulos = df_limpo.select(count(when(col(coluna_escaped).isNull(), 1))).collect()[0][0]\n",
    "\n",
    "    porcentagem_nulos = (qtd_nulos / total_linhas) * 100\n",
    "\n",
    "    if porcentagem_nulos > (limite_nulos * 100):\n",
    "        print(f\"Removendo {coluna}: {porcentagem_nulos:.2f}% de nulos (incluindo não classificados convertidos).\")\n",
    "        colunas_a_remover.append(coluna)\n",
    "        continue\n",
    "\n",
    "    # 2. Verifica valores repetitivos (>95%)\n",
    "    repeticao_maxima = df_limpo.groupBy(coluna_escaped).count() \\\n",
    "        .agg({\"count\": \"max\"}).collect()[0][0]\n",
    "\n",
    "    if repeticao_maxima is None: repeticao_maxima = 0\n",
    "\n",
    "    porcentagem_maxima = (repeticao_maxima / total_linhas) * 100\n",
    "\n",
    "    if porcentagem_maxima > (limite_repeticao * 100):\n",
    "        print(f\"Removendo {coluna}: {porcentagem_maxima:.2f}% de valor repetido.\")\n",
    "        colunas_a_remover.append(coluna)\n",
    "\n",
    "if colunas_a_remover:\n",
    "    df_final = df_limpo.drop(*colunas_a_remover)\n",
    "    print(f\"\\nTotal removido: {len(colunas_a_remover)}\")\n",
    "else:\n",
    "    df_final = df_limpo\n",
    "    print(\"\\nNenhuma coluna removida.\")\n",
    "\n",
    "df_limpo.unpersist()\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEMRtWI1wYO_"
   },
   "source": [
    "### 2.4 Remoção de Colunas Altamente Correlacionada\n",
    "Serão identificadas e removidas colunas que possuem conteúdo idêntico ou derivado direto de outras colunas existentes, eliminando redundâncias desnecessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGV-V3RC1VrJ",
    "outputId": "8e9c9d22-0517-453f-cfa1-ed1026f6d735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agente  causador  acidente\n",
      "data acidente\n",
      "cbo\n",
      "cid-\n",
      "cnae2.0 empregador\n",
      "cnae2.0 empregador descrição\n",
      "emitente cat\n",
      "espécie do benefício\n",
      "filiação segurado\n",
      "indica óbito acidente\n",
      "munic empr\n",
      "natureza da lesão\n",
      "origem de cadastramento cat\n",
      "parte corpo atingida\n",
      "sexo\n",
      "tipo do acidente\n",
      "uf  munic.  acidente\n",
      "uf munic. empregador\n",
      "data acidente\n",
      "Removendo coluna:  data acidente\n",
      "data despacho benefício\n",
      "data acidente\n",
      "Removendo coluna:  data acidente\n",
      "data nascimento\n",
      "data emissão cat\n",
      "cnpj/cei empregador\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Agente  Causador  Acidente='Ataque de Ser Vivo por Mordedura, Picada, Chi', Data Acidente1='2023/04', CBO='621005-Trab. Agropecuário em Geral', CID-10='M23.8 Outr Transt Internos do Joelho', CNAE2.0 Empregador=None, CNAE2.0 Empregador Descrição=None, Munic Empr='000000-Ignorado', Natureza da Lesão='Lesao Imediata', Parte Corpo Atingida='Joelho', Sexo='Masculino', Tipo do Acidente='Típico', UF  Munic.  Acidente='Zerado', UF Munic. Empregador='Zerado', Data Nascimento='05/06/1977', Data Emissão CAT='01/05/2023')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tirar_sufixo_numerico(coluna):\n",
    "  return re.sub(r'\\d+$', '', coluna)  # Apenas sufixo\n",
    "\n",
    "colunas_a_remover = []\n",
    "colunas_limpas = set()\n",
    "\n",
    "for coluna in df.columns:\n",
    "  nome_limpo = tirar_sufixo_numerico(coluna).lower()\n",
    "  print(nome_limpo)\n",
    "\n",
    "  if nome_limpo in colunas_limpas:\n",
    "    print(\"Removendo coluna: \", nome_limpo)\n",
    "    colunas_a_remover.append(coluna)\n",
    "  else:\n",
    "    colunas_limpas.add(nome_limpo)\n",
    "\n",
    "df_final = df_final.drop(*colunas_a_remover)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-XzwZBeshtz"
   },
   "source": [
    "### 2.5 Tratando/Limpando valores nulos\n",
    "\n",
    "O tratamento dos campos nulos foi definido com base no impacto analítico de cada variável:\n",
    "\n",
    "(Agente Causador, CBO e UF. Munic. do Acidente): Valores nulos serão alterados para o termo \"Não identificado\". Isso preserva o registro para contagem total, já que outras variáveis permitem análises parciais\n",
    "\n",
    "(CID e CNAE): São campos essenciais. Sem o CID, perde-se a causa médica; sem o CNAE, inviabiliza-se a análise por setor econômicos. Se nulos, serão descartados, pois a falta desses dados inviabiliza a análise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYVyFhBEbJEB",
    "outputId": "0031e7fa-e5af-4817-dc6b-c2ee1e2e14d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total antes: 157845\n",
      "Total depois: 145692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Agente  Causador  Acidente='Rua e Estrada - Superficie Utilizada para Sus', Data Acidente1='2023/03', CBO='252105-Administrador', CID-10='S42.0 Frat da Clavicula', CNAE2.0 Empregador=4711, CNAE2.0 Empregador Descrição='Comercio Varejista de Mercadorias em Geral, c', Munic Empr='354990-São José dos Campos', Natureza da Lesão='Fratura', Parte Corpo Atingida='Ombro', Sexo='Masculino', Tipo do Acidente='Trajeto', UF  Munic.  Acidente='Maranhão', UF Munic. Empregador='São Paulo', Data Nascimento='05/06/1977', Data Emissão CAT='01/05/2023')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_para_preencher = [\"Agente  Causador  Acidente\", \"CBO\", \"`UF  Munic.  Acidente`\"]\n",
    "cols_criticas_para_drop = [\"CID-10\", \"`CNAE2.0 Empregador`\", \"`CNAE2.0 Empregador Descrição`\"]\n",
    "\n",
    "df_tratado = df_final \\\n",
    "    .na.fill(\"Não identificado\", subset=cols_para_preencher) \\\n",
    "    .na.drop(subset=cols_criticas_para_drop)\n",
    "\n",
    "print(f\"Total antes: {df_final.count()}\")\n",
    "print(f\"Total depois: {df_tratado.count()}\")\n",
    "\n",
    "df_tratado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvP78cJ91XJP"
   },
   "source": [
    "## 3. Carregamento"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
